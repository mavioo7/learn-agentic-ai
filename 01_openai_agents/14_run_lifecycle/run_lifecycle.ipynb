{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdKwzEluDBN7"
      },
      "source": [
        "# Install openai-agents SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3QdkOviEB2ay",
        "outputId": "8475636b-66fb-43d9-b393-b75e135b8f44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.0/599.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yD91lz4DIAx"
      },
      "source": [
        "# Make your Notebook capable of running asynchronous functions.\n",
        "Both Jupyter notebooks and Python’s asyncio library utilize event loops, but they serve different purposes and can sometimes interfere with each other.\n",
        "\n",
        "The nest_asyncio library allows the existing event loop to accept nested event loops, enabling asyncio code to run within environments that already have an event loop, such as Jupyter notebooks.\n",
        "\n",
        "In summary, both Jupyter notebooks and Python’s asyncio library utilize event loops to manage asynchronous operations. When working within Jupyter notebooks, it’s essential to be aware of the existing event loop to effectively run asyncio code without conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C8YXyIpiZ9v4"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQsVowow7ihQ"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XnusaX_RWF22"
      },
      "outputs": [],
      "source": [
        "from agents import (\n",
        "    AsyncOpenAI,\n",
        "    OpenAIChatCompletionsModel,\n",
        ")\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oPvcFwItoKqw"
      },
      "outputs": [],
      "source": [
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "# Check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y9LkW-F7nC3T"
      },
      "outputs": [],
      "source": [
        "from agents import set_default_openai_client, set_tracing_disabled\n",
        "\n",
        "set_default_openai_client(external_client)\n",
        "set_tracing_disabled(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWL7EnI_7mIF"
      },
      "source": [
        "# Learning LifeCycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xL1SE0WBzNfB"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import asyncio\n",
        "import random\n",
        "from typing import Any\n",
        "\n",
        "from agents import Agent, RunContextWrapper, RunHooks, Runner, Tool, Usage, function_tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1dd25XzEBp"
      },
      "source": [
        "### 1. Basic Example (Understand Core Concept)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gfScTeRmzQYS"
      },
      "outputs": [],
      "source": [
        "class TestHooks(RunHooks):\n",
        "    def __init__(self):\n",
        "        self.event_counter = 0\n",
        "        self.name = \"TestHooks\"\n",
        "\n",
        "    async def on_agent_start(self, context: RunContextWrapper, agent: Agent) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(f\"### {self.name} {self.event_counter}: Agent {agent.name} started. Usage: {context.usage}\")\n",
        "\n",
        "    async def on_agent_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(f\"### {self.name} {self.event_counter}: Agent {agent.name} ended. Usage: {context.usage}, Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzedWWWrzfQD",
        "outputId": "210664aa-c624-4316-9e38-b89f5c83cc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### TestHooks 1: Agent Content Moderator Agent started. Usage: Usage(requests=0, input_tokens=0, output_tokens=0, total_tokens=0)\n",
            "### TestHooks 2: Agent Content Moderator Agent ended. Usage: Usage(requests=1, input_tokens=43, output_tokens=134, total_tokens=177), Output: This tweet asks a question about the economic effects of tariffs. While I can't provide a definitive \"good\" or \"bad\" answer as it's a complex topic with varying viewpoints, I can offer information.\n",
            "\n",
            "**Response to User:**\n",
            "\n",
            "\"Tariffs can be a complex topic in economics! Generally, they're taxes on imported goods. Some argue they can protect domestic industries and create jobs. Others say they raise prices for consumers and can lead to trade wars. It really depends on the specific situation and who you ask. There's lots of info online about the pros and cons from different perspectives if you'd like to learn more!\"\n",
            "\n",
            "This tweet asks a question about the economic effects of tariffs. While I can't provide a definitive \"good\" or \"bad\" answer as it's a complex topic with varying viewpoints, I can offer information.\n",
            "\n",
            "**Response to User:**\n",
            "\n",
            "\"Tariffs can be a complex topic in economics! Generally, they're taxes on imported goods. Some argue they can protect domestic industries and create jobs. Others say they raise prices for consumers and can lead to trade wars. It really depends on the specific situation and who you ask. There's lots of info online about the pros and cons from different perspectives if you'd like to learn more!\"\n",
            "\n",
            "--end--\n"
          ]
        }
      ],
      "source": [
        "start_hook = TestHooks()\n",
        "\n",
        "start_agent = Agent(\n",
        "    name=\"Content Moderator Agent\",\n",
        "    instructions=\"You are content moderation agent. Watch social media content received and flag queries that need help or answer. We will answer anything about AI?\",\n",
        "    model=model\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(\n",
        "      start_agent,\n",
        "      hooks=start_hook,\n",
        "      input=f\"<tweet>why tarrif is good or not good for economy.</tweet>\"\n",
        "  )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "asyncio.run(main())\n",
        "print(\"--end--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqF1gaVZ2R0l"
      },
      "source": [
        "We can add callbacks on various lifecycle events in an agent run listed here:\n",
        "\n",
        "https://openai.github.io/openai-agents-python/ref/lifecycle/#agents.lifecycle.RunHooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM8n3d4GzGGz"
      },
      "source": [
        "### 2. Advanced Example (With Tools and Agents HandOff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2XzWlsI2yue2"
      },
      "outputs": [],
      "source": [
        "class ExampleHooks(RunHooks):\n",
        "    def __init__(self):\n",
        "        self.event_counter = 0\n",
        "\n",
        "    def _usage_to_str(self, usage: Usage) -> str:\n",
        "        return f\"{usage.requests} requests, {usage.input_tokens} input tokens, {usage.output_tokens} output tokens, {usage.total_tokens} total tokens\"\n",
        "\n",
        "    async def on_agent_start(self, context: RunContextWrapper, agent: Agent) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### {self.event_counter}: Agent {agent.name} started. Usage: {self._usage_to_str(context.usage)}\"\n",
        "        )\n",
        "\n",
        "    async def on_agent_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### {self.event_counter}: Agent {agent.name} ended with output {output}. Usage: {self._usage_to_str(context.usage)}\"\n",
        "        )\n",
        "\n",
        "    async def on_tool_start(self, context: RunContextWrapper, agent: Agent, tool: Tool) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### {self.event_counter}: Tool {tool.name} started. Usage: {self._usage_to_str(context.usage)}\"\n",
        "        )\n",
        "\n",
        "    async def on_tool_end(\n",
        "        self, context: RunContextWrapper, agent: Agent, tool: Tool, result: str\n",
        "    ) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### {self.event_counter}: Tool {tool.name} ended with result {result}. Usage: {self._usage_to_str(context.usage)}\"\n",
        "        )\n",
        "\n",
        "    async def on_handoff(\n",
        "        self, context: RunContextWrapper, from_agent: Agent, to_agent: Agent\n",
        "    ) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### {self.event_counter}: Handoff from {from_agent.name} to {to_agent.name}. Usage: {self._usage_to_str(context.usage)}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S11ItAh-resv",
        "outputId": "eb22ce0b-5efe-4997-f629-4cfd4356c151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a max number: 68\n",
            "### 1: Agent Start Agent started. Usage: 0 requests, 0 input tokens, 0 output tokens, 0 total tokens\n",
            "### 2: Tool random_number started. Usage: 1 requests, 74 input tokens, 4 output tokens, 78 total tokens\n",
            "### 3: Tool random_number ended with result 37. Usage: 1 requests, 74 input tokens, 4 output tokens, 78 total tokens\n",
            "### 4: Handoff from Start Agent to Multiply Agent. Usage: 2 requests, 158 input tokens, 27 output tokens, 185 total tokens\n",
            "### 5: Agent Multiply Agent started. Usage: 2 requests, 158 input tokens, 27 output tokens, 185 total tokens\n",
            "### 6: Tool multiply_by_two started. Usage: 3 requests, 243 input tokens, 46 output tokens, 289 total tokens\n",
            "### 7: Tool multiply_by_two ended with result 74. Usage: 3 requests, 243 input tokens, 46 output tokens, 289 total tokens\n",
            "### 8: Agent Multiply Agent ended with output The final result is 74.\n",
            ". Usage: 4 requests, 355 input tokens, 55 output tokens, 410 total tokens\n",
            "Final Output is: \n",
            "The final result is 74.\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "hooks = ExampleHooks()\n",
        "\n",
        "\n",
        "@function_tool(\"random_number\")\n",
        "def random_number(max: int) -> int:\n",
        "    \"\"\"Generate a random number up to the provided max.\"\"\"\n",
        "    return random.randint(0, max)\n",
        "\n",
        "\n",
        "@function_tool(\"multiply_by_two\")\n",
        "def multiply_by_two(x: int) -> int:\n",
        "    \"\"\"Return x times two.\"\"\"\n",
        "    return x * 2\n",
        "\n",
        "@function_tool(\"divide_by_two\")\n",
        "def divide_by_two(x: int) -> int:\n",
        "    \"\"\"Return x divided by two.\"\"\"\n",
        "    return x / 2\n",
        "\n",
        "\n",
        "divider_agent = Agent(\n",
        "    name=\"Divider Agent\",\n",
        "    instructions=\"Divide the number by 2 and return the final result.\",\n",
        "    tools=[divide_by_two],\n",
        "    model=model\n",
        ")\n",
        "\n",
        "multiply_agent = Agent(\n",
        "    name=\"Multiply Agent\",\n",
        "    instructions=\"Multiply the number by 2 and if it more then 100 hand off to divider agent else return the final result.\",\n",
        "    tools=[multiply_by_two],\n",
        "    model=model\n",
        ")\n",
        "\n",
        "start_agent = Agent(\n",
        "    name=\"Start Agent\",\n",
        "    instructions=\"Generate a random number. If it's even, stop. If it's odd, hand off to the multipler agent.\",\n",
        "    tools=[random_number],\n",
        "    handoffs=[multiply_agent],\n",
        "    model=model\n",
        ")\n",
        "\n",
        "\n",
        "async def main() -> None:\n",
        "    user_input = input(\"Enter a max number: \")\n",
        "    ans = await Runner.run(\n",
        "        start_agent,\n",
        "        hooks=hooks,\n",
        "        input=f\"Generate a random number between 0 and {user_input}.\",\n",
        "    )\n",
        "    print(\"Final Output is: \")\n",
        "    print(ans.final_output)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}